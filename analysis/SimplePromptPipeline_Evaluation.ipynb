{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a65eeb9",
   "metadata": {},
   "source": [
    "# Evaluating SimplePromptPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root (the directory that contains \"src\")\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# --- Imports from your project ---\n",
    "from src.pipelines.SimplePromptPipeline import SimplePromptPipeline\n",
    "from src.app.Evaluator import Evaluator\n",
    "from src.llms.LLM_Wrappers import AbstractLLM\n",
    "\n",
    "# --- Define Models to Evaluate ---\n",
    "models = {\n",
    "    \"GPT-3\": AbstractLLM.from_name(\"gpt-3.5-turbo\"),\n",
    "    \"GPT-4o-mini\": AbstractLLM.from_name(model_name=\"gpt-4o-mini\"),\n",
    "    \"GPT-4o\": AbstractLLM.from_name(model_name=\"gpt-4o\"),\n",
    "}\n",
    "\n",
    "# --- Files to Evaluate ---\n",
    "ground_truth_files = [\n",
    "    \"../src/data/Q17_Annotated_Responses.json\",\n",
    "    # \"../src/data/Q19_Annotated_Responses.json\",\n",
    "    # \"../src/data/Q20_Annotated_Responses.json\",\n",
    "    # \"../src/data/Q21_Annotated_Responses.json\",\n",
    "    # \"../src/data/Q22_Annotated_Responses.json\",\n",
    "]\n",
    "\n",
    "# --- Output and Results Directories ---\n",
    "output_dir = \"../outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "results_dir = os.path.join(\"../analysis\", \"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# --- Parameters ---\n",
    "min_conf = 0.7\n",
    "records = []\n",
    "\n",
    "# --- Run pipelines + evaluate ---\n",
    "for model_name, llm in models.items():\n",
    "    print(f\"\\n=== Running {model_name} ===\")\n",
    "    for gt_path in ground_truth_files:\n",
    "        qname = os.path.basename(gt_path).replace(\"_Annotated_Responses.json\", \"\")\n",
    "        print(f\" â†’ Processing {qname}...\")\n",
    "\n",
    "        # Run pipeline\n",
    "        pipeline = SimplePromptPipeline(llm=llm, input_path=gt_path, output_dir=output_dir)\n",
    "        maybe_path = pipeline.run()\n",
    "\n",
    "        # Create model-specific filename\n",
    "        model_suffix = model_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "        base_name = os.path.splitext(os.path.basename(gt_path))[0]\n",
    "        model_output_name = f\"{base_name}_{model_suffix}_annotated.json\"\n",
    "        output_path = os.path.join(output_dir, model_output_name)\n",
    "\n",
    "        # If pipeline didn't create that, rename existing _annotated.json file if present\n",
    "        if not os.path.exists(output_path):\n",
    "            generic_output = os.path.join(output_dir, f\"{base_name}_annotated.json\")\n",
    "            if os.path.exists(generic_output):\n",
    "                os.rename(generic_output, output_path)\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Expected output file not found: {output_path}\")\n",
    "\n",
    "        # Evaluate results\n",
    "        evaluator = Evaluator(output_path, gt_path)\n",
    "        results = evaluator.evaluate_precision_recall(min_confidence=min_conf)\n",
    "        global_metrics = results[\"global\"]\n",
    "\n",
    "        records.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Question\": qname,\n",
    "            \"Precision\": global_metrics[\"precision\"],\n",
    "            \"Recall\": global_metrics[\"recall\"],\n",
    "            \"F1-Score\": global_metrics[\"f1-score\"],\n",
    "        })\n",
    "\n",
    "# --- Build DataFrames ---\n",
    "df = pd.DataFrame(records)\n",
    "summary_df = (\n",
    "    df.groupby(\"Model\")[[\"Precision\", \"Recall\", \"F1-Score\"]]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    "      .sort_values(\"Model\")\n",
    ")\n",
    "\n",
    "# --- Save Results ---\n",
    "df.to_csv(os.path.join(results_dir, \"per_question_results.csv\"), index=False)\n",
    "summary_df.to_csv(os.path.join(results_dir, \"summary_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b89770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== ðŸ“Š Per-Question Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_9af38\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9af38_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_9af38_level0_col1\" class=\"col_heading level0 col1\" >Question</th>\n",
       "      <th id=\"T_9af38_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_9af38_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_9af38_level0_col4\" class=\"col_heading level0 col4\" >F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9af38_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_9af38_row0_col0\" class=\"data row0 col0\" >GPT-3</td>\n",
       "      <td id=\"T_9af38_row0_col1\" class=\"data row0 col1\" >Q17</td>\n",
       "      <td id=\"T_9af38_row0_col2\" class=\"data row0 col2\" >0.362</td>\n",
       "      <td id=\"T_9af38_row0_col3\" class=\"data row0 col3\" >0.602</td>\n",
       "      <td id=\"T_9af38_row0_col4\" class=\"data row0 col4\" >0.452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9af38_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_9af38_row1_col0\" class=\"data row1 col0\" >GPT-4o-mini</td>\n",
       "      <td id=\"T_9af38_row1_col1\" class=\"data row1 col1\" >Q17</td>\n",
       "      <td id=\"T_9af38_row1_col2\" class=\"data row1 col2\" >0.417</td>\n",
       "      <td id=\"T_9af38_row1_col3\" class=\"data row1 col3\" >0.759</td>\n",
       "      <td id=\"T_9af38_row1_col4\" class=\"data row1 col4\" >0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9af38_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_9af38_row2_col0\" class=\"data row2 col0\" >GPT-4o</td>\n",
       "      <td id=\"T_9af38_row2_col1\" class=\"data row2 col1\" >Q17</td>\n",
       "      <td id=\"T_9af38_row2_col2\" class=\"data row2 col2\" >0.457</td>\n",
       "      <td id=\"T_9af38_row2_col3\" class=\"data row2 col3\" >0.759</td>\n",
       "      <td id=\"T_9af38_row2_col4\" class=\"data row2 col4\" >0.570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1680f0150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== ðŸ§® Average Precision/Recall per Model ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_016f0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_016f0_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_016f0_level0_col1\" class=\"col_heading level0 col1\" >Precision</th>\n",
       "      <th id=\"T_016f0_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_016f0_level0_col3\" class=\"col_heading level0 col3\" >F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_016f0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_016f0_row0_col0\" class=\"data row0 col0\" >GPT-3</td>\n",
       "      <td id=\"T_016f0_row0_col1\" class=\"data row0 col1\" >0.362</td>\n",
       "      <td id=\"T_016f0_row0_col2\" class=\"data row0 col2\" >0.602</td>\n",
       "      <td id=\"T_016f0_row0_col3\" class=\"data row0 col3\" >0.452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_016f0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_016f0_row1_col0\" class=\"data row1 col0\" >GPT-4o</td>\n",
       "      <td id=\"T_016f0_row1_col1\" class=\"data row1 col1\" >0.457</td>\n",
       "      <td id=\"T_016f0_row1_col2\" class=\"data row1 col2\" >0.759</td>\n",
       "      <td id=\"T_016f0_row1_col3\" class=\"data row1 col3\" >0.570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_016f0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_016f0_row2_col0\" class=\"data row2 col0\" >GPT-4o-mini</td>\n",
       "      <td id=\"T_016f0_row2_col1\" class=\"data row2 col1\" >0.417</td>\n",
       "      <td id=\"T_016f0_row2_col2\" class=\"data row2 col2\" >0.759</td>\n",
       "      <td id=\"T_016f0_row2_col3\" class=\"data row2 col3\" >0.538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16a74c390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Display Results (with graceful fallback if jinja2 missing) ---\n",
    "print(\"\\n\\n=== ðŸ“Š Per-Question Results ===\")\n",
    "try:\n",
    "    display(df.style.format({\"Precision\": \"{:.3f}\", \"Recall\": \"{:.3f}\", \"F1-Score\": \"{:.3f}\"}))\n",
    "except AttributeError:\n",
    "    print(df.to_string(index=False, formatters={\"Precision\": \"{:.3f}\".format, \"Recall\": \"{:.3f}\".format, \"F1-Score\": \"{:.3f}\".format}))\n",
    "\n",
    "print(\"\\n\\n=== ðŸ§® Average Precision/Recall per Model ===\")\n",
    "try:\n",
    "    display(summary_df.style.format({\"Precision\": \"{:.3f}\", \"Recall\": \"{:.3f}\", \"F1-Score\": \"{:.3f}\"}))\n",
    "except AttributeError:\n",
    "    print(summary_df.to_string(index=False, formatters={\"Precision\": \"{:.3f}\".format, \"Recall\": \"{:.3f}\".format, \"F1-Score\": \"{:.3f}\".format}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6211ce2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thematicLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
