{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a65eeb9",
   "metadata": {},
   "source": [
    "# Evaluating SimplePromptPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9949195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running GPT-3 ===\n",
      " â†’ Processing Q17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotating entries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [01:29<00:00,  1.17entry/s, Last: 1.32s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated JSON written to ../outputs/Q17_Annotated_Responses_annotated.json\n",
      "Logs saved to logs/Q17_Annotated_Responses_20251103_101049.log\n",
      "\n",
      "=== Running GPT-4o-mini ===\n",
      " â†’ Processing Q17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotating entries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [02:26<00:00,  1.41s/entry, Last: 1.64s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated JSON written to ../outputs/Q17_Annotated_Responses_annotated.json\n",
      "Logs saved to logs/Q17_Annotated_Responses_20251103_101218.log\n",
      "\n",
      "=== Running GPT-4o ===\n",
      " â†’ Processing Q17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotating entries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [01:57<00:00,  1.13s/entry, Last: 1.78s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated JSON written to ../outputs/Q17_Annotated_Responses_annotated.json\n",
      "Logs saved to logs/Q17_Annotated_Responses_20251103_101444.log\n",
      "\n",
      "\n",
      "=== ðŸ“Š Per-Question Results ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "The '.style' accessor requires jinja2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m/var/folders/gr/7ln67xd15qd1lr8v52br0zm00000gn/T/ipykernel_65416/3077787321.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     86\u001b[39m       .sort_values(\u001b[33m\"Model\"\u001b[39m)\n\u001b[32m     87\u001b[39m )\n\u001b[32m     88\u001b[39m \n\u001b[32m     89\u001b[39m print(\u001b[33m\"\\n\\n=== ðŸ“Š Per-Question Results ===\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m display(df.style.format({\u001b[33m\"Precision\"\u001b[39m: \u001b[33m\"{:.3f}\"\u001b[39m, \u001b[33m\"Recall\"\u001b[39m: \u001b[33m\"{:.3f}\"\u001b[39m}))\n\u001b[32m     91\u001b[39m \n\u001b[32m     92\u001b[39m print(\u001b[33m\"\\n\\n=== ðŸ§® Average Precision/Recall per Model ===\"\u001b[39m)\n\u001b[32m     93\u001b[39m display(summary_df.style.format({\u001b[33m\"Precision\"\u001b[39m: \u001b[33m\"{:.3f}\"\u001b[39m, \u001b[33m\"Recall\"\u001b[39m: \u001b[33m\"{:.3f}\"\u001b[39m}))\n",
      "\u001b[32m/opt/anaconda3/envs/thematicLM/lib/python3.11/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6317\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6318\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6319\u001b[39m         ):\n\u001b[32m   6320\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6321\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[32m/opt/anaconda3/envs/thematicLM/lib/python3.11/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1445\u001b[39m         \"\"\"\n\u001b[32m   1446\u001b[39m         \u001b[38;5;66;03m# Raise AttributeError so that inspect works even if jinja2 is not installed.\u001b[39;00m\n\u001b[32m   1447\u001b[39m         has_jinja2 = import_optional_dependency(\u001b[33m\"jinja2\"\u001b[39m, errors=\u001b[33m\"ignore\"\u001b[39m)\n\u001b[32m   1448\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m has_jinja2:\n\u001b[32m-> \u001b[39m\u001b[32m1449\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\u001b[33m\"The '.style' accessor requires jinja2\"\u001b[39m)\n\u001b[32m   1450\u001b[39m \n\u001b[32m   1451\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.io.formats.style \u001b[38;5;28;01mimport\u001b[39;00m Styler\n\u001b[32m   1452\u001b[39m \n",
      "\u001b[31mAttributeError\u001b[39m: The '.style' accessor requires jinja2"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root (the directory that contains \"src\")\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# --- Imports from your project ---\n",
    "from src.pipelines.SimplePromptPipeline import SimplePromptPipeline\n",
    "from src.app.Evaluator import Evaluator\n",
    "from src.llms.LLM_Wrappers import AbstractLLM\n",
    "\n",
    "# --- Define Models to Evaluate ---\n",
    "# Assuming AbstractLLM is subclassed for OpenAI-like models\n",
    "# If you have specific wrappers like GPT3LLM, replace AbstractLLM() calls accordingly\n",
    "models = {\n",
    "    \"GPT-3\": AbstractLLM.from_name(\"gpt-3.5-turbo\"),\n",
    "    \"GPT-4o-mini\": AbstractLLM.from_name(model_name=\"gpt-4o-mini\"),\n",
    "    \"GPT-4o\": AbstractLLM.from_name(model_name=\"gpt-4o\"),\n",
    "}\n",
    "\n",
    "# --- Files to Evaluate ---\n",
    "ground_truth_files = [\n",
    "    \"../src/data/Q17_Annotated_Responses.json\",\n",
    "    # \"../src/data/Q19_Annotated_Responses.json\",\n",
    "    # \"../src/data/Q20_Annotated_Responses.json\",\n",
    "    # \"../src/data/Q21_Annotated_Responses.json\",\n",
    "    # \"../src/data/Q22_Annotated_Responses.json\",\n",
    "]\n",
    "\n",
    "# --- Output directory ---\n",
    "output_dir = \"../outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Parameters ---\n",
    "min_conf = 0.7\n",
    "records = []  # will hold all evaluation rows\n",
    "\n",
    "# --- Run pipelines + evaluate ---\n",
    "for model_name, llm in models.items():\n",
    "    print(f\"\\n=== Running {model_name} ===\")\n",
    "    for gt_path in ground_truth_files:\n",
    "        qname = os.path.basename(gt_path).replace(\"_Annotated_Responses.json\", \"\")\n",
    "        print(f\" â†’ Processing {qname}...\")\n",
    "\n",
    "        # Run pipeline\n",
    "        pipeline = SimplePromptPipeline(llm=llm, input_path=gt_path, output_dir=output_dir)\n",
    "        # Run the pipeline (sometimes returns None even if file is written)\n",
    "        maybe_path = pipeline.run()\n",
    "\n",
    "        # Robustly determine the actual output path\n",
    "        if maybe_path and os.path.exists(maybe_path):\n",
    "            output_path = maybe_path\n",
    "        else:\n",
    "            # Reconstruct expected path from AbstractTAPipeline logic\n",
    "            base_name = os.path.splitext(os.path.basename(gt_path))[0]\n",
    "            output_path = os.path.join(output_dir, f\"{base_name}_annotated.json\")\n",
    "            if not os.path.exists(output_path):\n",
    "                raise FileNotFoundError(f\"Expected output file not found: {output_path}\")\n",
    "\n",
    "        # Evaluate results\n",
    "        evaluator = Evaluator(output_path, gt_path)\n",
    "\n",
    "        results = evaluator.evaluate_precision_recall(min_confidence=min_conf)\n",
    "        global_metrics = results[\"global\"]\n",
    "\n",
    "        records.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Question\": qname,\n",
    "            \"Precision\": global_metrics[\"precision\"],\n",
    "            \"Recall\": global_metrics[\"recall\"]\n",
    "        })\n",
    "\n",
    "# --- Build DataFrames ---\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Compute average metrics across all questions per model\n",
    "summary_df = (\n",
    "    df.groupby(\"Model\")[[\"Precision\", \"Recall\"]]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    "      .sort_values(\"Model\")\n",
    ")\n",
    "\n",
    "print(\"\\n\\n=== ðŸ“Š Per-Question Results ===\")\n",
    "display(df.style.format({\"Precision\": \"{:.3f}\", \"Recall\": \"{:.3f}\"}))\n",
    "\n",
    "print(\"\\n\\n=== ðŸ§® Average Precision/Recall per Model ===\")\n",
    "display(summary_df.style.format({\"Precision\": \"{:.3f}\", \"Recall\": \"{:.3f}\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6211ce2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thematicLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
